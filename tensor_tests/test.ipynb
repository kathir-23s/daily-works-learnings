{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b38c70ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ActivationFunctions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmlp_latest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mActivationFunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ActivationFunction\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mTensorT\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_scratch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorT\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmlp_latest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MLP\n",
      "File \u001b[0;32m~/Code_Works/mlp_latest/mlp/mlp.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mActivationFunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ActivationFunction\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mWeightInitialization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WeightInitializer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mLossFunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LossFunction\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ActivationFunctions'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from mlp_latest.mlp.ActivationFunctions import ActivationFunction\n",
    "from TensorT.tensor_scratch import TensorT\n",
    "from mlp_latest.mlp.mlp import MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c50f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: tensor:\n",
      "[[5, 12],\n",
      " [21, 32]], shape: (2, 2)\n",
      "Result shape: (2, 2)\n",
      "Result data: [[5, 12], [21, 32]]\n",
      "Operation: mul\n",
      "Parents: (tensor:\n",
      "[[1, 2],\n",
      " [3, 4]], shape: (2, 2), tensor:\n",
      "[[5, 6],\n",
      " [7, 8]], shape: (2, 2))\n"
     ]
    }
   ],
   "source": [
    "a = TensorT([[1, 2], [3, 4]])\n",
    "b = TensorT([[5, 6], [7, 8]])\n",
    "\n",
    "c = a * b  # calls modified __add__\n",
    "\n",
    "# Check result data\n",
    "print(\"Result:\", c)\n",
    "print(\"Result shape:\", c.shape)      # Should print (2, 2)\n",
    "print(\"Result data:\", c.data)\n",
    "\n",
    "# Check graph info\n",
    "print(\"Operation:\", c._op)           # Should print 'add'\n",
    "print(\"Parents:\", c._parent)        # Should show (a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e7eeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient w.r.t a:\n",
      "[[1.0, 1.0], [1.0, 1.0]]\n",
      "Gradient w.r.t b:\n",
      "[[-1.0, -1.0], [-1.0, -1.0]]\n"
     ]
    }
   ],
   "source": [
    "a = TensorT([[1, 2], [3, 4]])\n",
    "b = TensorT([[5, 6], [7, 8]])\n",
    "\n",
    "a.grad = None\n",
    "b.grad = None\n",
    "# Call backward on c with default grad (i.e., dL/dc=1)\n",
    "# c = a.tmatmul(b) \n",
    "c = a-b\n",
    "# Should create computation graph node with backward_fn defined\n",
    "c.backward()\n",
    "\n",
    "print(\"Gradient w.r.t a:\")\n",
    "print(a.grad)  # Expect all ones matrix (same shape as a) because d(c)/d(a)=1 elementwise\n",
    "\n",
    "print(\"Gradient w.r.t b:\")\n",
    "print(b.grad)  # Same as above\n",
    "\n",
    "# print(\"Gradient w.r.t c:\")\n",
    "# print(c.grad)\n",
    "# print(\"----\")\n",
    "# print(c.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e11b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor:\n",
      "[[6],\n",
      " [15]], shape: (2, 1)\n",
      "tensor:\n",
      "[[5, 7, 9]], shape: (1, 3)\n",
      "tensor:\n",
      "[[0, 2],\n",
      " [3, 0]], shape: (2, 2)\n",
      "tensor:\n",
      "[[-500, 2],\n",
      " [3, 500]], shape: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "# Test tsum_axis\n",
    "t = TensorT([[1, 2, 3], [4, 5, 6]])  # shape (2,3)\n",
    "print(t.tsum_axis(axis=1, keepdims=True))  # Should be [[6], [15]] shape (2,1)\n",
    "print(t.tsum_axis(axis=0, keepdims=True))  # Should be [[5, 7, 9]] shape (1,3)\n",
    "\n",
    "# Test tmaximum  \n",
    "t2 = TensorT([[-1, 2], [3, -4]])\n",
    "print(t2.tmaximum(0))  # Should be [[0, 2], [3, 0]] - ReLU effect\n",
    "\n",
    "# Test tclip\n",
    "t3 = TensorT([[-600, 2], [3, 600]])  \n",
    "print(t3.tclip(-500, 500))  # Should be [[-500, 2], [3, 500]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702f0a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1 backward calls: a.grad=[[1.0]], b.grad=[[1.0]]\n",
      "After 2 backward calls: a.grad=[[2.0]], b.grad=[[2.0]]\n",
      "After 3 backward calls: a.grad=[[3.0]], b.grad=[[3.0]]\n",
      "After 4 backward calls: a.grad=[[4.0]], b.grad=[[4.0]]\n",
      "After 5 backward calls: a.grad=[[5.0]], b.grad=[[5.0]]\n"
     ]
    }
   ],
   "source": [
    "a = TensorT([[2.0]])\n",
    "b = TensorT([[3.0]])\n",
    "c = a + b\n",
    "\n",
    "for i in range(5):   # 5 backward calls\n",
    "    c.backward()\n",
    "    print(f\"After {i+1} backward calls: a.grad={a.grad}, b.grad={b.grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88111373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Activation Functions ===\n",
      "Input: [[0.5, 0.0], [-1.0, 2.0], [0.25, -0.456]]\n",
      "Input tensor ID: 139774007851136\n",
      "\n",
      "--- ReLU Test ---\n",
      "Output: [[0.5, 0], [0, 2.0], [0.25, 0]]\n",
      "Expected: [[0.5, 0], [0, 2.0], [0.25, 0]]\n",
      "Op tracked: 'relu'\n",
      "Parent ID matches: True\n",
      "Gradient: ([[1.0, 0], [0, 1.0]],)\n",
      "Expected gradient: [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]\n",
      "\n",
      "--- Sigmoid Test ---\n",
      "Output: tensor:\n",
      "[[0.6224593312018546, 0.5],\n",
      " [0.2689414213699951, 0.8807970779778823],\n",
      " [0.5621765008857981, 0.3879351629339386]], shape: (3, 2)\n",
      "NumPy expected: [[0.62245933 0.5       ]\n",
      " [0.26894142 0.88079708]\n",
      " [0.5621765  0.38793516]]\n",
      "Values match: True\n",
      "Op tracked: 'sigmoid'\n",
      "Parent ID matches: True\n",
      "Gradient: [[[0.23500371 0.25      ]\n",
      "  [0.19661193 0.10499359]]]\n",
      "Expected gradient: [[0.23500371 0.25      ]\n",
      " [0.19661193 0.10499359]\n",
      " [0.24613408 0.23744147]]\n",
      "\n",
      "--- Tanh Test ---\n",
      "Output: [[ 0.46211716  0.        ]\n",
      " [-0.76159416  0.96402758]\n",
      " [ 0.24491866 -0.4268185 ]]\n",
      "NumPy expected: [[ 0.46211716  0.        ]\n",
      " [-0.76159416  0.96402758]\n",
      " [ 0.24491866 -0.4268185 ]]\n",
      "Values match: True\n",
      "Op tracked: 'tanh'\n",
      "Parent ID matches: True\n",
      "Gradient: [[[0.78644773 1.        ]\n",
      "  [0.41997434 0.07065082]]]\n",
      "Expected gradient: [[0.78644773 1.        ]\n",
      " [0.41997434 0.07065082]\n",
      " [0.94001485 0.81782597]]\n",
      "\n",
      "--- Graph Structure Test ---\n",
      "All operations tracked correctly: True\n",
      "All parents point to same input: True\n",
      "All have backward functions: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "# from tensor_scratch import TensorT\n",
    "\n",
    "def test_activations():\n",
    "    print(\"=== Testing Activation Functions ===\")\n",
    "    \n",
    "    # Test input\n",
    "    input_data = [[0.5, 0.0], [-1.0, 2.0], [0.25, -0.456]]\n",
    "    z = TensorT(input_data)\n",
    "    print(f\"Input: {input_data}\")\n",
    "    print(f\"Input tensor ID: {id(z)}\")\n",
    "    print()\n",
    "    \n",
    "    # Test ReLU\n",
    "    print(\"--- ReLU Test ---\")\n",
    "    relu_out = ActivationFunction.relu(z)\n",
    "    print(f\"Output: {relu_out.data}\")\n",
    "    print(f\"Expected: {[[max(0, x) for x in row] for row in input_data]}\")\n",
    "    print(f\"Op tracked: '{relu_out._op}'\")\n",
    "    print(f\"Parent ID matches: {id(z) == id(relu_out._parent[0])}\")\n",
    "    \n",
    "    # Test gradient\n",
    "    grad_input = [[1.0, 1.0], [1.0, 1.0]]  # Ones gradient from upstream\n",
    "    relu_grad = relu_out.backward_fn(grad_input)\n",
    "    expected_relu_grad = [[1.0 if x > 0 else 0.0 for x in row] for row in input_data]\n",
    "    print(f\"Gradient: {relu_grad}\")\n",
    "    print(f\"Expected gradient: {expected_relu_grad}\")\n",
    "    print()\n",
    "    \n",
    "    # Test Sigmoid\n",
    "    print(\"--- Sigmoid Test ---\")\n",
    "    sigmoid_out = ActivationFunction.sigmoid(z)\n",
    "    numpy_sigmoid = 1 / (1 + np.exp(-np.array(input_data)))\n",
    "    # print(f\"Output: {np.array(sigmoid_out.data)}\")\n",
    "    print(f\"Output: {sigmoid_out}\")\n",
    "    print(f\"NumPy expected: {numpy_sigmoid}\")\n",
    "    print(f\"Values match: {np.allclose(sigmoid_out.data, numpy_sigmoid, atol=1e-6)}\")\n",
    "    print(f\"Op tracked: '{sigmoid_out._op}'\")\n",
    "    print(f\"Parent ID matches: {id(z) == id(sigmoid_out._parent[0])}\")\n",
    "    \n",
    "    # Test sigmoid gradient  \n",
    "    sigmoid_grad = sigmoid_out.backward_fn(grad_input)\n",
    "    # Expected: grad * sigmoid_out * (1 - sigmoid_out)\n",
    "    s_vals = np.array(sigmoid_out.data)\n",
    "    expected_sigmoid_grad = s_vals * (1 - s_vals)\n",
    "    print(f\"Gradient: {np.array(sigmoid_grad)}\")\n",
    "    print(f\"Expected gradient: {expected_sigmoid_grad}\")\n",
    "    print()\n",
    "    \n",
    "    # Test Tanh\n",
    "    print(\"--- Tanh Test ---\")\n",
    "    tanh_out = ActivationFunction.tanh(z)\n",
    "    numpy_tanh = np.tanh(np.array(input_data))\n",
    "    print(f\"Output: {np.array(tanh_out.data)}\")\n",
    "    print(f\"NumPy expected: {numpy_tanh}\")\n",
    "    print(f\"Values match: {np.allclose(tanh_out.data, numpy_tanh, atol=1e-6)}\")\n",
    "    print(f\"Op tracked: '{tanh_out._op}'\")\n",
    "    print(f\"Parent ID matches: {id(z) == id(tanh_out._parent[0])}\")\n",
    "    \n",
    "    # Test tanh gradient\n",
    "    tanh_grad = tanh_out.backward_fn(grad_input)\n",
    "    # Expected: grad * (1 - tanh²(x))\n",
    "    t_vals = np.array(tanh_out.data)\n",
    "    expected_tanh_grad = 1 - t_vals**2\n",
    "    print(f\"Gradient: {np.array(tanh_grad)}\")\n",
    "    print(f\"Expected gradient: {expected_tanh_grad}\")\n",
    "    print()\n",
    "    \n",
    "    # Test computational graph structure\n",
    "    print(\"--- Graph Structure Test ---\")\n",
    "    print(f\"All operations tracked correctly: {all([relu_out._op == 'relu',sigmoid_out._op == 'sigmoid',tanh_out._op == 'tanh'])}\")\n",
    "\n",
    "    print(f\"All parents point to same input: {all([relu_out._parent[0] is z,sigmoid_out._parent[0] is z,tanh_out._parent[0] is z])}\")\n",
    "\n",
    "    print(f\"All have backward functions: {all([relu_out.backward_fn is not None,sigmoid_out.backward_fn is not None,tanh_out.backward_fn is not None])}\")\n",
    "# Run the test\n",
    "test_activations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca554c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_loss_functions():\n",
    "#     \"\"\"Test all loss functions with simple 2x3 tensors\"\"\"\n",
    "    \n",
    "#     # Create test data\n",
    "#     y_true = TensorT([[0, 1, 0], [1, 0, 1]])\n",
    "#     y_pred = TensorT([[0.1, 0.8, 0.1], [0.9, 0.2, 0.7]])\n",
    "    \n",
    "#     print(\"Testing all loss functions...\")\n",
    "    \n",
    "#     # Test Cross Entropy Loss\n",
    "#     try:\n",
    "#         ce_loss = LossFunction.cross_entropy_loss(y_true, y_pred)\n",
    "#         print(f\"✓ Cross Entropy Loss: {ce_loss.data}\")\n",
    "#         grad_ce = ce_loss.backward_fn([[1.0]])\n",
    "#         print(f\"  Gradients computed: {type(grad_ce)}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"✗ Cross Entropy Loss failed: {e}\")\n",
    "    \n",
    "#     # Test Mean Squared Error\n",
    "#     try:\n",
    "#         mse_loss = LossFunction.mean_squared_error(y_true, y_pred)\n",
    "#         print(f\"✓ Mean Squared Error: {mse_loss.data}\")\n",
    "#         grad_mse = mse_loss.backward_fn([[1.0]])\n",
    "#         print(f\"  Gradients computed: {type(grad_mse)}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"✗ Mean Squared Error failed: {e}\")\n",
    "    \n",
    "#     # Test Categorical Cross Entropy\n",
    "#     try:\n",
    "#         cat_ce_loss = LossFunction.categorical_cross_entropy_loss(y_true, y_pred)\n",
    "#         print(f\"✓ Categorical Cross Entropy: {cat_ce_loss.data}\")\n",
    "#         grad_cat = cat_ce_loss.backward_fn([[1.0]])\n",
    "#         print(f\"  Gradients computed: {type(grad_cat)}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"✗ Categorical Cross Entropy failed: {e}\")\n",
    "    \n",
    "#     # Test Hinge Loss\n",
    "#     try:\n",
    "#         hinge_loss = LossFunction.hinge_loss(y_true, y_pred)\n",
    "#         print(f\"✓ Hinge Loss: {hinge_loss.data}\")\n",
    "#         grad_hinge = hinge_loss.backward_fn([[1.0]])\n",
    "#         print(f\"  Gradients computed: {type(grad_hinge)}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"✗ Hinge Loss failed: {e}\")\n",
    "    \n",
    "#     # Test Binary Cross Entropy\n",
    "#     try:\n",
    "#         bce_loss = LossFunction.binary_cross_entropy_loss(y_true, y_pred)\n",
    "#         print(f\"✓ Binary Cross Entropy: {bce_loss.data}\")\n",
    "#         grad_bce = bce_loss.backward_fn([[1.0]])\n",
    "#         print(f\"  Gradients computed: {type(grad_bce)}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"✗ Binary Cross Entropy failed: {e}\")\n",
    "    \n",
    "#     # Test Mean Absolute Error\n",
    "#     try:\n",
    "#         mae_loss = LossFunction.mean_absolute_error(y_true, y_pred)\n",
    "#         print(f\"✓ Mean Absolute Error: {mae_loss.data}\")\n",
    "#         grad_mae = mae_loss.backward_fn([[1.0]])\n",
    "#         print(f\"  Gradients computed: {type(grad_mae)}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"✗ Mean Absolute Error failed: {e}\")\n",
    "    \n",
    "#     # Test get() method\n",
    "#     try:\n",
    "#         loss_fn, grad_fn = LossFunction.get('mean_squared_error')\n",
    "#         print(f\"✓ get() method works: {loss_fn.__name__}, {grad_fn}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"✗ get() method failed: {e}\")\n",
    "    \n",
    "#     print(\"\\nAll loss functions have backward_fn attribute:\")\n",
    "#     for name in ['cross_entropy_loss', 'mean_squared_error', 'categorical_cross_entropy_loss', \n",
    "#                 'hinge_loss', 'binary_cross_entropy_loss', 'mean_absolute_error']:\n",
    "#         loss_result = getattr(LossFunction, name)(y_true, y_pred)\n",
    "#         has_backward = hasattr(loss_result, 'backward_fn')\n",
    "#         print(f\"  {name}: {has_backward}\")\n",
    "\n",
    "# # Run the test\n",
    "# test_loss_functions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c54028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid shape: (2, 2)\n",
      "Softmax shape: (2, 2)\n",
      "Column sums: [0.9999999999999999, 0.9999999999999999]\n",
      "ReLU output: [[0, 2.0], [0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# Test sigmoid\n",
    "z = TensorT([[1.0, -1.0], [2.0, -2.0]])\n",
    "sig_out = ActivationFunction.sigmoid(z)\n",
    "print(\"Sigmoid shape:\", sig_out.shape)  # Should be (2, 2)\n",
    "\n",
    "# Test softmax  \n",
    "z = TensorT([[1.0, 2.0], [3.0, 4.0]])\n",
    "soft_out = ActivationFunction.softmax(z)\n",
    "print(\"Softmax shape:\", soft_out.shape)  # Should be (2, 2)\n",
    "# Column sums should be approximately 1\n",
    "col_sums = [sum(col) for col in zip(*soft_out.data)]\n",
    "print(\"Column sums:\", col_sums)  # Should be close to [1.0, 1.0]\n",
    "\n",
    "# Test ReLU\n",
    "z = TensorT([[-1.0, 2.0], [0.0, -3.0]])\n",
    "relu_out = ActivationFunction.relu(z)\n",
    "print(\"ReLU output:\", relu_out.data)  # Should be [[0.0, 2.0], [0.0, 0.0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e831fa38",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MLP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test 1: Check types after initialization\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mlp \u001b[38;5;241m=\u001b[39m \u001b[43mMLP\u001b[49m(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, hidden_layers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m], output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeight 1 type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(mlp\u001b[38;5;241m.\u001b[39mweights[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Should be <class 'TensorT'>\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBias 1 type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(mlp\u001b[38;5;241m.\u001b[39mbiases)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)    \u001b[38;5;66;03m# Should be <class 'TensorT'>\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MLP' is not defined"
     ]
    }
   ],
   "source": [
    "# Test 1: Check types after initialization\n",
    "mlp = MLP(input_size=3, hidden_layers=[4, 2], output_size=1)\n",
    "print(f\"Weight 1 type: {type(mlp.weights[0])}\")  # Should be <class 'TensorT'>\n",
    "print(f\"Bias 1 type: {type(mlp.biases)}\")    # Should be <class 'TensorT'>\n",
    "\n",
    "# Test 2: Check shapes\n",
    "print(f\"Weight 1 shape: {mlp.weights.shape}\")  # Should be (4, 3)\n",
    "print(f\"Bias 1 shape: {mlp.biases.shape}\")    # Should be (4, 1)\n",
    "\n",
    "# Test 3: Check bias values are zero\n",
    "print(f\"Bias 1 data: {mlp.biases.data}\")      # Should be [[0.0], [0.0], [0.0], [0.0]]\n",
    "\n",
    "# Test 4: Verify all layers\n",
    "for i, (w, b) in enumerate(zip(mlp.weights, mlp.biases)):\n",
    "    print(f\"Layer {i+1}: W{w.shape}, b{b.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
