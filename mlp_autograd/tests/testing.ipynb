{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4463843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.mlp import MLP\n",
    "from tensor.tensor_scratch import TensorT\n",
    "from tensor.tensor_scratch import TensorT\n",
    "from core.ActivationFunctions import ActivationFunction\n",
    "from core.LossFunctions import LossFunction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f2d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Forward test ---\n",
    "# logits: 3 classes, 2 samples\n",
    "logits = TensorT([[2.0, 1.0],\n",
    "                  [1.0, 3.0],\n",
    "                  [0.1, 0.2]])\n",
    "\n",
    "softmax_out = ActivationFunction.softmax(logits)\n",
    "\n",
    "print(\"Softmax Output:\")\n",
    "for row in softmax_out.data:\n",
    "    print(row)\n",
    "\n",
    "# Each column (sample) should sum to 1\n",
    "print(\"Column sums:\", [sum(col) for col in zip(*softmax_out.data)])\n",
    "\n",
    "\n",
    "# --- Backward test ---\n",
    "# pretend gradient from loss is all ones (shape matches output)\n",
    "grad_out = [[1.42195, -4.46124],\n",
    "            [5.42451, 4.57461],\n",
    "            [-8.43795, 5.52499]]\n",
    "\n",
    "softmax_out.backward(grad=grad_out)\n",
    "\n",
    "print(\"\\nGradient wrt logits:\")\n",
    "for row in logits.grad:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c57db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# logits: 3 classes x 2 samples (same as before)\n",
    "logits = TensorT([[2.0, 1.0],\n",
    "                  [1.0, 3.0],\n",
    "                  [0.1, 0.2]])\n",
    "S = ActivationFunction.softmax(logits)  # (3,2)\n",
    "\n",
    "# one-hot labels per column: targets [class0, class1]\n",
    "Y = TensorT([[1, 0],   # class 0 for sample 1\n",
    "             [0, 1],   # class 1 for sample 2\n",
    "             [0, 0]])\n",
    "\n",
    "m = S.shape[1]  # batch size (=2)\n",
    "\n",
    "# Upstream gradient from CE w.r.t probabilities: dL/dS = -(Y / S) / m\n",
    "# (we're simulating CE backward input to softmax)\n",
    "grad_wrt_probs = []\n",
    "for i in range(S.shape[0]):\n",
    "    row = []\n",
    "    for j in range(S.shape[1]):\n",
    "        s_ij = S.data[i][j]\n",
    "        y_ij = Y.data[i][j]\n",
    "        row.append(-(y_ij / max(s_ij, 1e-15)) / m)\n",
    "    grad_wrt_probs.append(row)\n",
    "\n",
    "# Backprop through softmax with our simulated upstream\n",
    "S.backward(grad=grad_wrt_probs)\n",
    "\n",
    "# Expected gradient at logits: (S - Y) / m\n",
    "expected = []\n",
    "for i in range(S.shape[0]):\n",
    "    row = []\n",
    "    for j in range(S.shape[1]):\n",
    "        row.append((S.data[i][j] - Y.data[i][j]) / m)\n",
    "    expected.append(row)\n",
    "\n",
    "print(\"Grad wrt logits (computed):\")\n",
    "for r in logits.grad: print(r)\n",
    "print(\"\\nExpected (S - Y)/m:\")\n",
    "for r in expected: print(r)\n",
    "\n",
    "# Quick numeric check (small tolerance)\n",
    "ok = True\n",
    "tol = 1e-8\n",
    "for i in range(len(expected)):\n",
    "    for j in range(len(expected[0])):\n",
    "        if abs(logits.grad[i][j] - expected[i][j]) > tol:\n",
    "            ok = False\n",
    "print(\"\\nMatch within tolerance? ->\", ok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbcd6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3 classes, 1 sample (column)\n",
    "logits = TensorT([[2.0],\n",
    "                  [1.0],\n",
    "                  [0.1]])\n",
    "S = ActivationFunction.softmax(logits)\n",
    "\n",
    "# Upstream grad that nudges class 0 only (like loss pushing true class up)\n",
    "grad_wrt_probs = [[-1.0], [0.0], [0.0]]  # shape (3,1)\n",
    "S.backward(grad=grad_wrt_probs)\n",
    "\n",
    "print(\"Softmax probs:\", [row[0] for row in S.data])     # just to see s\n",
    "print(\"Grad wrt logits:\", [row[0] for row in logits.grad])\n",
    "# Expect: class 0 negative; other classes positive (mass shifts from others to the true class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7182000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensor.tensor_scratch import TensorT\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa399f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_old(z: TensorT) -> TensorT:\n",
    "    # ----- forward (numerically stable) -----\n",
    "    z_max_per_col = [max(col) for col in zip(*z.data)]\n",
    "    z_stable_data = []\n",
    "    for row in z.data:\n",
    "        z_stable_data.append([x - z_max_per_col[j] for j, x in enumerate(row)])\n",
    "\n",
    "    exp_data = z._apply_unary(z_stable_data, math.exp)\n",
    "    sum_exp_per_col = [sum(col) for col in zip(*exp_data)]\n",
    "\n",
    "    result_data = []\n",
    "    for row in exp_data:\n",
    "        result_data.append([x / sum_exp_per_col[j] for j, x in enumerate(row)])\n",
    "\n",
    "    out = TensorT(result_data, _op='softmax_old', _parent=(z,))\n",
    "\n",
    "    # ----- backward (diag-only; BUGGY for multiclass) -----\n",
    "    def backward_fn(grad_op):\n",
    "        s = result_data  # (C, m)\n",
    "        grad_in = z._apply_elementwise(grad_op, s, lambda g, s_val: g * s_val * (1 - s_val))\n",
    "        return (grad_in,)\n",
    "    out.backward_fn = backward_fn\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2957e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_new(z: TensorT) -> TensorT:\n",
    "    # ----- forward (numerically stable) -----\n",
    "    z_max_per_col = [max(col) for col in zip(*z.data)]\n",
    "    z_stable_data = []\n",
    "    for row in z.data:\n",
    "        z_stable_data.append([x - z_max_per_col[j] for j, x in enumerate(row)])\n",
    "\n",
    "    exp_data = z._apply_unary(z_stable_data, math.exp)\n",
    "    sum_exp_per_col = [sum(col) for col in zip(*exp_data)]\n",
    "\n",
    "    result_data = []\n",
    "    for row in exp_data:\n",
    "        result_data.append([x / sum_exp_per_col[j] for j, x in enumerate(row)])\n",
    "\n",
    "    out = TensorT(result_data, _op='softmax_new', _parent=(z,))\n",
    "\n",
    "    # ----- backward (CORRECT VJP) -----\n",
    "    def backward_fn(grad_op):\n",
    "        s = result_data                  # (C, m) nested lists\n",
    "        C = len(s)\n",
    "        M = len(s[0]) if C else 0\n",
    "        grad_in = [[0.0 for _ in range(M)] for _ in range(C)]\n",
    "        for j in range(M):\n",
    "            dot = sum(grad_op[i][j] * s[i][j] for i in range(C))  # g^T s\n",
    "            for i in range(C):\n",
    "                grad_in[i][j] = s[i][j] * (grad_op[i][j] - dot)\n",
    "        return (grad_in,)\n",
    "    out.backward_fn = backward_fn\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e34f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits: 3 classes x 2 samples\n",
    "Z = TensorT([[2.0, 1.0],\n",
    "             [1.0, 3.0],\n",
    "             [0.1, 0.2]])\n",
    "\n",
    "S_old = softmax_old(Z)\n",
    "S_new = softmax_new(Z)\n",
    "\n",
    "print(\"Old softmax column sums:\", [sum(col) for col in zip(*S_old.data)])\n",
    "print(\"New softmax column sums:\", [sum(col) for col in zip(*S_new.data)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01892dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nOld Softmax Output:\", S_old.data)\n",
    "print(\"New Softmax Output:\", S_new.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9002316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upstream grad of ones\n",
    "G = [[1.0, 1.0],\n",
    "     [1.0, 1.0],\n",
    "     [1.0, 1.0]]\n",
    "\n",
    "# old\n",
    "Z1 = TensorT([[2.0, 1.0],[1.0, 3.0],[0.1, 0.2]])\n",
    "S1 = softmax_old(Z1)\n",
    "S1.backward(grad=G)\n",
    "print(\"Old: grad wrt logits (ones upstream):\")\n",
    "for r in Z1.grad: print(r)\n",
    "\n",
    "# new\n",
    "Z2 = TensorT([[2.0, 1.0],[1.0, 3.0],[0.1, 0.2]])\n",
    "S2 = softmax_new(Z2)\n",
    "S2.backward(grad=G)\n",
    "print(\"\\nNew: grad wrt logits (ones upstream):\")\n",
    "for r in Z2.grad: print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e474c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits again\n",
    "Z = TensorT([[2.0, 1.0],\n",
    "             [1.0, 3.0],\n",
    "             [0.1, 0.2]])\n",
    "\n",
    "# one-hot labels per column: sample1->class0, sample2->class1\n",
    "Y = TensorT([[1, 0],\n",
    "             [0, 1],\n",
    "             [0, 0]])\n",
    "\n",
    "# -------- OLD --------\n",
    "Z_old = TensorT([row[:] for row in Z.data])\n",
    "S_old = softmax_old(Z_old)\n",
    "\n",
    "m = S_old.shape[1]\n",
    "# upstream from CE: dL/dS = -(Y / S) / m   (elementwise)\n",
    "G_old = []\n",
    "for i in range(S_old.shape[0]):\n",
    "    row = []\n",
    "    for j in range(S_old.shape[1]):\n",
    "        s_ij = S_old.data[i][j]\n",
    "        y_ij = Y.data[i][j]\n",
    "        row.append(-(y_ij / max(s_ij, 1e-15)) / m)\n",
    "    G_old.append(row)\n",
    "\n",
    "S_old.backward(grad=G_old)\n",
    "\n",
    "# expected logits grad: (S - Y)/m\n",
    "expected_old = [[(S_old.data[i][j] - Y.data[i][j]) / m\n",
    "                 for j in range(S_old.shape[1])]\n",
    "                for i in range(S_old.shape[0])]\n",
    "\n",
    "print(\"OLD softmax: grad wrt logits\")\n",
    "for r in Z_old.grad: print(r)\n",
    "print(\"\\nExpected (S - Y)/m (using OLD S):\")\n",
    "for r in expected_old: print(r)\n",
    "\n",
    "\n",
    "# -------- NEW --------\n",
    "Z_new = TensorT([row[:] for row in Z.data])\n",
    "S_new = softmax_new(Z_new)\n",
    "\n",
    "m = S_new.shape[1]\n",
    "G_new = []\n",
    "for i in range(S_new.shape[0]):\n",
    "    row = []\n",
    "    for j in range(S_new.shape[1]):\n",
    "        s_ij = S_new.data[i][j]\n",
    "        y_ij = Y.data[i][j]\n",
    "        row.append(-(y_ij / max(s_ij, 1e-15)) / m)\n",
    "    G_new.append(row)\n",
    "\n",
    "S_new.backward(grad=G_new)\n",
    "\n",
    "expected_new = [[(S_new.data[i][j] - Y.data[i][j]) / m\n",
    "                 for j in range(S_new.shape[1])]\n",
    "                for i in range(S_new.shape[0])]\n",
    "\n",
    "print(\"\\nNEW softmax: grad wrt logits\")\n",
    "for r in Z_new.grad: print(r)\n",
    "print(\"\\nExpected (S - Y)/m (using NEW S):\")\n",
    "for r in expected_new: print(r)\n",
    "\n",
    "# quick check flags\n",
    "tol = 1e-8\n",
    "def close(A,B):\n",
    "    for i in range(len(A)):\n",
    "        for j in range(len(A[0])):\n",
    "            if abs(A[i][j]-B[i][j])>tol: return False\n",
    "    return True\n",
    "\n",
    "print(\"\\nMATCH OLD?  \", close(Z_old.grad, expected_old))\n",
    "print(\"MATCH NEW?  \", close(Z_new.grad, expected_new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb50644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single sample (3x1), upstream nudges class 0 only\n",
    "Z_a = TensorT([[2.0],[1.0],[0.1]])\n",
    "S_a_old = softmax_old(Z_a)\n",
    "S_a_old.backward(grad=[[-1.0],[0.0],[0.0]])\n",
    "print(\"OLD grad wrt logits (3x1):\", [r[0] for r in Z_a.grad])\n",
    "\n",
    "Z_b = TensorT([[2.0],[1.0],[0.1]])\n",
    "S_b_new = softmax_new(Z_b)\n",
    "S_b_new.backward(grad=[[-1.0],[0.0],[0.0]])\n",
    "print(\"NEW grad wrt logits (3x1):\", [r[0] for r in Z_b.grad])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e47c59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.0, -50.0, -2.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b, c, d = 10, 3, 3, 5\n",
    "\n",
    "lhs = -a * b / c / d\n",
    "rhs = -a * b / c * d\n",
    "mhs = (-a * b) / (c * d)\n",
    "\n",
    "lhs, rhs, mhs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
